@article{liu2023summary,
  title={Summary of chatgpt-related research and perspective towards the future of large language models},
  author={Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and others},
  journal={Meta-Radiology},
  pages={100017},
  year={2023},
  publisher={Elsevier}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{spitale2023ai,
  title={AI model GPT-3 (dis) informs us better than humans},
  author={Spitale, Giovanni and Biller-Andorno, Nikola and Germani, Federico},
  journal={Science Advances},
  volume={9},
  number={26},
  pages={eadh1850},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{tian2023chatgpt,
  title={Is ChatGPT the ultimate programming assistant--how far is it?},
  author={Tian, Haoye and Lu, Weiqi and Li, Tsz On and Tang, Xunzhu and Cheung, Shing-Chi and Klein, Jacques and Bissyand{\'e}, Tegawend{\'e} F},
  journal={arXiv preprint arXiv:2304.11938},
  year={2023}
}
@online{chatgpt2023,
  title   = {The inside story of how ChatGPT was built from the people who made it},
  organization = {MIT Technolog Review},
  author  = {Will Douglas Heavenarchive},
  year    = 2023,
  url     = {https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/},
  urldate = {2024-08-01}
}
@article{zheng2023large,
  title={Large language models for scientific synthesis, inference and explanation},
  author={Zheng, Yizhen and Koh, Huan Yee and Ju, Jiaxin and Nguyen, Anh TN and May, Lauren T and Webb, Geoffrey I and Pan, Shirui},
  journal={arXiv preprint arXiv:2310.07984},
  year={2023}
}
@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@article{zhang2023language,
  title={Language Models are Universal Embedders},
  author={Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min},
  journal={arXiv preprint arXiv:2310.08232},
  year={2023}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{rubenstein1965contextual,
  title={Contextual correlates of synonymy},
  author={Rubenstein, Herbert and Goodenough, John B},
  journal={Communications of the ACM},
  volume={8},
  number={10},
  pages={627--633},
  year={1965},
  publisher={ACM New York, NY, USA}
}
@inproceedings{heimerl2018interactive,
  title={Interactive analysis of word vector embeddings},
  author={Heimerl, Florian and Gleicher, Michael},
  booktitle={Computer Graphics Forum},
  volume={37},
  number={3},
  pages={253--265},
  year={2018},
  organization={Wiley Online Library}
}

@inproceedings{huang-etal-2012-improving,
    title = "Improving Word Representations via Global Context and Multiple Word Prototypes",
    author = "Huang, Eric  and
      Socher, Richard  and
      Manning, Christopher  and
      Ng, Andrew",
    editor = "Li, Haizhou  and
      Lin, Chin-Yew  and
      Osborne, Miles  and
      Lee, Gary Geunbae  and
      Park, Jong C.",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-1092",
    pages = "873--882",
}

@article{ji2022speeding,
  title={Speeding Up Question Answering Task of Language Models via Inverted Index},
  author={Ji, Xiang and Sungu-Eryilmaz, Yesim and Momeni, Elaheh and Rawassizadeh, Reza},
  journal={arXiv preprint arXiv:2210.13578},
  year={2022}
}

@article{Chatterjee2017Index,
  title={Index and Indexing},
  author={A. Chatterjee},
  year={2017},
  pages={125-134},
  doi={10.1016/B978-0-08-102025-8.00009-0}
}

@article{Vickery1950THE,
  title={THE STRUCTURE OF A CONNECTIVE INDEX},
  author={B. Vickery},
  journal={Journal of Documentation},
  year={1950},
  volume={6},
  pages={140-151},
  doi={10.1108/EB026158}
}

@article{Lo2016What,
  title={What Is an Index?},
  author={A. Lo},
  journal={The Journal of Portfolio Management},
  year={2016},
  volume={42},
  pages={21 -36},
  doi={10.3905/jpm.2016.42.2.021}
}

@inproceedings{rahutomo2012semantic,
  title={Semantic cosine similarity},
  author={Rahutomo, Faisal and Kitasuka, Teruaki and Aritsugi, Masayoshi and others},
  booktitle={The 7th international student conference on advanced science and technology ICAST},
  volume={4},
  number={1},
  pages={1},
  year={2012},
  organization={University of Seoul South Korea}
}
@misc{treesitter,
  title   = {Using Parsers},
  url     = {https://tree-sitter.github.io/tree-sitter/using-parsers},
  urldate = {2024-08-08}
}

@misc{embeddingmodel,
  title   = {New and improved embedding model},
  publisher = {OpenAI},
  url     = {https://openai.com/index/new-and-improved-embedding-model/},
  urldate = {2024-08-08}
}

@misc{completionmodel,
  title   = {Azure OpenAI Service-Modelle},
  publisher = {OpenAI},
  url     = {https://learn.microsoft.com/de-de/azure/ai-services/openai/concepts/models#gpt-35},
  urldate = {2024-08-08}
}

@article{ouyang2023llm,
  title={LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation},
  author={Ouyang, Shuyin and Zhang, Jie M and Harman, Mark and Wang, Meng},
  journal={arXiv preprint arXiv:2308.02828},
  year={2023}
}